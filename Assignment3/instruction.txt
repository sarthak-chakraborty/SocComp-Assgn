#################################
	SARTHAK CHAKRABORTY
	16CS30044
#################################

## TASK 1 ##
1. Run `python main.py` to get the predictions stored in `predictions` directory.

2. For FastText, the algorithm creates an input file `task1/fasttext_train/file.train` which is used by the library.

3. I have used `python3.7` for the purpose of this assignment.

4. If a virtual environment is created, make sure these libraries are installed:
	blis==0.4.1
	catalogue==1.0.0
	certifi==2020.6.20
	chardet==3.0.4
	click==7.1.2
	cymem==2.0.4
	en-core-web-md @ https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz
	fasttext==0.9.2
	idna==2.10
	importlib-metadata==2.0.0
	joblib==0.17.0
	murmurhash==1.0.4
	numpy==1.19.4
	pandas==1.1.4
	plac==1.1.3
	preshed==3.0.4
	pybind11==2.6.0
	python-dateutil==2.8.1
	pytz==2020.4
	regex==2020.10.28
	requests==2.24.0
	scikit-learn==0.23.2
	scipy==1.5.4
	six==1.15.0
	spacy==2.3.2
	srsly==1.0.3
	thinc==7.4.1
	threadpoolctl==2.1.0
	tqdm==4.51.0
	urllib3==1.25.11
	wasabi==0.8.0
	zipp==3.4.0


## TASK 2 ##
1. The model that I have used is a LSTM based model followed by a dense layer for prediction of the class.
   The LSTM model has 50 hidden states followed by a dropout of 0.5 and uses cross entropy loss function along with adam optimizer.
   The model uses 50 dimensional Glove Word2Vec embeddings for the input and performs cross validation while training.

2. The model is inspired from the github repository: https://github.com/pinkeshbadjatiya/twitter-hatespeech.git

3. The Glove embedding file (zip) can be downloaded from the link: https://drive.google.com/file/d/14zYjuvPNN3QcOACDFLgSIFDMb23yRwvn/view?usp=sharing

4. Place the zip file in `task2/glove-emb/` directory and unzip it.

5. Install the requirements: `pip install -r requirements.txt`. Use `python3.7` for the assignment.

6. Run `python lstm.py -f ./glove-emb/glove.twitter.27B.50d.txt -d 50 --tokenizer glove --loss categorical_crossentropy --optimizer adam --initialize-weights random --learn-embeddings --epochs 15 --batch-size 512`

7. It takes around 30 mins to train and store results.